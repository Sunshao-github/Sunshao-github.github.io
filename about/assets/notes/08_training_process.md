# 🎯 模型训练：从“文盲”到“专家”

## **整体流程**

text

预训练（学知识） → 监督微调（学做事） → RLHF（学做人）

## **第一步：预训练**

### **做什么**

- 让模型大量阅读，学习语言规律
    
- 像给小学生教语文数学
    

### **关键任务**

- **掩码语言模型**：完形填空
    
    text
    
    原句：我今天去__公园
    任务：预测“__”处的词
    
- **下一词预测**：根据上文预测
    
    text
    
    上文：春眠不觉晓
    预测：处处闻啼鸟
    

### **数据量**

- 文本量：万亿级token（单词片段）
    
- 时间：几周到几个月
    
- 算力：数千张GPU
    

### **成果**

- 获得语言能力
    
- 掌握基本知识
    
- 但不会“听话”
    

## **第二步：监督微调**

### **做什么**

- 教模型理解指令并执行
    
- 像大学生学专业技能
    

### **数据格式**

text

指令：写一首关于春天的诗
回复：春风吹拂万物苏，百花齐放映日红...

指令：将句子翻译成英文
回复：Translate the sentence into English

### **关键点**

- 数据质量 > 数据数量
    
- 需要人工编写或筛选
    
- 覆盖多种任务类型
    

### **成果**

- 能理解人类指令
    
- 能完成具体任务
    
- 但可能“不靠谱”
    

## **第三步：RLHF**

### **做什么**

- 通过人类反馈让模型更符合期望
    
- 像职场培训学为人处世
    

### **三个子步骤**

#### **1. 收集偏好数据**

- 给模型同一个问题的多个回答
    
- 人工标注哪个回答更好
    
- **例子**：
    
    text
    
    问：如何减肥？
    A：不吃不喝最快（错误）
    B：合理饮食+运动（正确）
    标注：B更好
    

#### **2. 训练奖励模型**

- 学习人类的偏好标准
    
- 能自动给回答打分
    
- **作用**：替代人工标注，节省成本
    

#### **3. 强化学习优化**

- 让模型学习获得高分的回答方式
    
- 不断调整参数
    
- **目标**：输出人类偏好的回答
    

## **完整比喻**

### **培养一个助理**

text

1. 预训练：送他上学（小学到大学）
   - 读很多书，学基础知识
   
2. 监督微调：岗前培训
   - 学公司流程，具体工作技能
   
3. RLHF：实际工作反馈
   - 老板说“这样做好”“那样做不好”
   - 逐渐适应公司文化

## **效果对比**

|阶段|能力|问题|
|---|---|---|
|预训练后|知识丰富|不听话，乱说|
|微调后|能执行指令|可能有害/不准确|
|RLHF后|安全、有用、诚实|有时过于保守|

## **训练成本**

- **预训练**：最贵，数百万美元
    
- **微调**：较便宜，数万到数十万美元
    
- **RLHF**：人工成本高，需持续进行