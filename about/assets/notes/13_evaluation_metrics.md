# 📊 如何给AI“考试打分”

## **核心问题**

> 怎么知道一个模型是好是坏？需要量化指标

## **两类评估指标**

### **1. 内部指标（训练时用）**

#### **困惑度**

- **是什么**：模型对文本的“困惑程度”
    
- **直观理解**：预测下一个词的难度
    
- **计算**：指数形式的平均负对数似然
    
- **解读**：
    
    - 困惑度越低越好
        
    - 人类文本的困惑度 ≈ 10-20
        
    - GPT-3的困惑度 ≈ 约10
        

#### **例子**

text

文本："今天天气很好"
- 差模型：很困惑（不知道"很好"常跟"天气"）→ 困惑度高
- 好模型：不困惑（知道常见搭配）→ 困惑度低

### **2. 外部指标（任务表现）**

#### **准确率**

- **适用**：分类任务（对/错）
    
- **计算**：正确数 / 总数
    
- **例子**：情感分析（正面/负面）
    

#### **精确率 & 召回率**

- **适用**：信息检索、分类
    
- **精确率**：找出的相关结果中，确实相关的比例
    
    text
    
    精确率 = 真阳性 / (真阳性 + 假阳性)
    
- **召回率**：所有相关结果中，找出的比例
    
    text
    
    召回率 = 真阳性 / (真阳性 + 假阴性)
    

#### **F1分数**

- **是什么**：精确率和召回率的调和平均
    
- **公式**：2 × (精确率×召回率)/(精确率+召回率)
    
- **作用**：平衡精确率和召回率
    

## **文本生成指标**

### **BLEU**

- **适用**：机器翻译
    
- **思想**：比较生成文本和参考文本的n-gram重叠
    
- **范围**：0-1，越高越好
    
- **例子**：
    
    text
    
    参考：the cat is on the mat
    生成：the cat is on the mat（完全一样） → BLEU=1.0
    生成：the cat sits on the mat（部分一样） → BLEU≈0.7
    

### **ROUGE**

- **适用**：文本摘要
    
- **类型**：
    
    - ROUGE-N：n-gram重叠
        
    - ROUGE-L：最长公共子序列
        
- **解读**：越高说明摘要越接近参考
    

## **大模型特有评估**

### **指令遵循率**

- 模型正确遵循指令的比例
    
- **例子**：
    
    text
    
    指令：用三句话总结
    评估：是否正好三句
    

### **安全性评分**

- 有害输出比例
    
- 拒绝不当请求的比例
    

### **创造力评估**

- 难以量化
    
- 方法：人工评分、多样性指标
    

## **评估流程示例**

### **翻译任务评估**

text

1. 准备测试集：1000句待翻译文本
2. 人工参考翻译：标准答案
3. 模型翻译：生成结果
4. 计算BLEU：比较生成和参考
5. 人工评估：流畅度、准确性

### **对话任务评估**

text

1. 多轮对话测试
2. 评估维度：
   - 相关性：回答是否相关
   - 连贯性：对话是否自然
   - 信息量：是否有用信息
   - 安全性：是否有害
3. 人工打分（1-5分）

## **评估挑战**

### **客观指标的问题**

1. **BLEU的局限**：同义不同词得分低
    
2. **多样性惩罚**：创造性回答可能得分低
    
3. **参考依赖**：依赖参考质量
    

### **主观评估的问题**

1. **不一致性**：不同人打分不同
    
2. **成本高**：需要大量人工
    
3. **可复现性差**：难以复现结果
    

## **最新趋势**

### **基于模型的评估**

- 用另一个AI模型评估
    
- 例子：GPT-4作为裁判评估其他模型
    
- 优点：快速、低成本
    
- 缺点：可能偏差
    

### **自动评估框架**

- HELM：全面评估语言模型
    
- BIG-bench：大规模评估基准
    
- MMLU：多任务语言理解评估
    

## **实用建议**

1. **综合评估**：不要只看一个指标
    
2. **任务相关**：选择适合任务的指标
    
3. **人工验证**：关键任务要人工检查
    
4. **持续监控**：上线后持续评估