# 📖 预训练：大模型的“基础教育”

## **一句话概括**

> 让模型通过“完形填空”大量阅读，学会语言规律和世界知识

## **核心任务：语言建模**

### **什么是语言建模**

- **目标**：根据前文预测下一个词
    
- **例子**：
    
    text
    
    输入：今天天气真__
    输出：好/不错/热...
    

### **两种方式**

#### **1. 自回归语言模型（GPT系列）**

- **方式**：从左到右预测
    
- **公式**：P(下一个词 | 前面所有词)
    
- **优点**：适合生成文本
    
- **缺点**：只能看到左边信息
    

#### **2. 掩码语言模型（BERT系列）**

- **方式**：遮盖某些词，根据上下文预测
    
- **例子**：
    
    text
    
    原句：我[MASK]去公园
    任务：预测“MASK”处的词（想/要/刚...）
    
- **优点**：能看到双向信息
    
- **缺点**：不适合直接生成
    

## **训练过程**

### **数据准备**

- **分词**：把文本切成小片段
    
- **批处理**：一批一批喂给模型
    
- **上下文长度**：一次看多少词（如2048个）
    

### **学习什么**

1. **语法规则**：主谓宾结构
    
2. **语义关系**：同义词、反义词
    
3. **事实知识**：北京是中国的首都
    
4. **推理能力**：如果A>B且B>C，则A>C
    

### **损失函数**

- **目标**：让预测越来越准
    
- **计算**：预测概率 vs 实际词
    
- **优化**：不断调整参数减少错误
    

## **关键技术**

### **1. 注意力机制**

- 让模型关注重要部分
    
- **例子**：
    
    text
    
    "苹果手机很贵"
    - "贵"应该关注"手机"，不是"苹果"
    

### **2. 位置编码**

- 告诉模型词的位置信息
    
- **重要性**：否则模型不知道词序
    

### **3. 层归一化**

- 稳定训练过程
    
- 防止梯度爆炸/消失
    

## **训练规模**

### **参数量级**

- 小模型：1亿参数（学基本语法）
    
- 中模型：10亿参数（学基本知识）
    
- 大模型：100亿+参数（出现涌现能力）
    

### **训练数据量**

- GPT-3：3000亿token
    
- 相当于：读完整个人类历史书籍很多遍
    
- 数据多样性：网页、书籍、代码、对话
    

## **涌现现象**

### **什么是涌现**

模型大到一定程度，突然会新技能

### **例子**

- 小模型：只能续写句子
    
- 大到一定程度：突然会翻译、写代码、推理
    

### **关键规模**

- 一般70亿参数开始出现
    
- 1000亿参数能力显著提升
    

## **成本与挑战**

### **训练成本**

- **硬件**：数千张A100/H100 GPU
    
- **时间**：几周到几个月
    
- **电费**：数百万美元
    
- **碳排放**：相当于几十辆汽车开一年
    

### **技术挑战**

1. **稳定性**：大规模训练容易崩溃
    
2. **效率**：如何更快训练
    
3. **内存**：模型太大放不下
    

## **预训练 vs 人类学习**

text

人类学习：
1. 婴儿：听大人说话
2. 学生：上学读书
3. 成人：持续学习

预训练：
1. 初始化：随机参数
2. 训练：读海量文本
3. 结果：掌握语言