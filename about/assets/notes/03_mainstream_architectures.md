# 🏗️ 大模型的“骨架”：Transformer

## **核心思想**

> 让模型像人一样：阅读时能“前后对照”，理解上下文关系

## **传统方法的局限**

### **RNN（循环神经网络）**

- **工作方式**：一个字一个字处理
    
- **问题**：记性差，处理到后面忘了前面
    
- **比喻**：抄电话号码，抄到后几位忘了前几位
    

### **CNN（卷积神经网络）**

- **工作方式**：看局部模式
    
- **问题**：适合图像，但理解句子关系差
    
- **比喻**：只看词语本身，不看词语间关系
    

## **Transformer的革命**

### **自注意力机制**

- **做什么**：计算每个词与其他词的关系
    
- **例子**：
    
    text
    
    句子：“苹果公司发布了新手机”
    - “苹果”看到“公司”→确定是企业
    - “手机”看到“发布”→确定是产品
    
- **优点**：能同时看整个句子，理解长距离关系
    

### **多头注意力**

- **比喻**：多个专家从不同角度分析
    
    - 专家1：关注语法关系
        
    - 专家2：关注语义关系
        
    - 专家3：关注上下文关系
        
- **结果**：理解更全面
    

## **两大派系**

### **1. GPT系列（生成派）**

- **特点**：只往前看，适合生成文本
    
- **工作**：像“完形填空”，根据上文预测下一个词
    
- **代表**：ChatGPT、GPT-4
    
- **优点**：生成流畅，对话自然
    

### **2. BERT系列（理解派）**

- **特点**：前后都看，适合理解
    
- **工作**：像“阅读理解”，能同时看到全文
    
- **代表**：BERT、RoBERTa
    
- **优点**：理解准确，适合分类任务
    

## **通俗类比**

### **GPT vs BERT**

text

GPT（文科生）：
- 考试：只给题目开头，续写作文
- 优势：创造力强，能一直写下去
- 缺点：可能跑题

BERT（理科生）：
- 考试：给完整文章，回答问题
- 优势：理解准确，答案靠谱
- 缺点：不会自己创作

## **T5架构（综合派）**

- **思想**：所有任务都转成“文本到文本”
    
- **例子**：
    
    - 翻译：输入英文→输出中文
        
    - 摘要：输入长文→输出短文
        
- **优点**：一个模型干所有事
    

## **关键公式（简化）**

text

注意力 = Softmax(Q·Kᵀ/√d)·V
- Q：我想知道什么（Query）
- K：你有什么信息（Key）
- V：具体内容是什么（Value）

## **为什么重要？**

1. **并行计算**：比RNN快很多
    
2. **长程依赖**：能记住很长的上下文
    
3. **扩展性强**：模型越大效果越好