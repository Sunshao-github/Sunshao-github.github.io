# 🐘 什么是大模型？

## **一句话定义**

> 参数超多的深度学习模型，像突然开窍一样展现出多种能力

## **三个关键特征**

### **1. 大参数（为什么叫“大”）**

- **参数是什么**：模型要学的“知识量”
    
- **对比一下**：
    
    - 早期模型：几百万参数（像小学生）
        
    - GPT-3：1750亿参数（像图书馆）
        
- **举例**：参数就像大脑的神经元连接，越多越能理解复杂东西
    

### **2. 预训练（怎么变聪明）**

- **过程**：先大量阅读→再专项学习
    
- **比喻**：
    
    - 预训练：读完整座图书馆（学通用知识）
        
    - 微调：专门学法律/医学（变成专家）
        
- **数据量**：训练数据 ≈ 几百万本书的内容
    

### **3. 涌现能力（神奇之处）**

- **现象**：大到一定程度，突然会新技能
    
- **例子**：
    
    - 小模型：只能续写句子
        
    - 大模型：突然会翻译、写诗、编程
        
- **关键**：不是设计出来的，是“自然出现”的
    

## **通俗理解**

### **传统AI vs 大模型**

text

传统AI：专科医生
- 训练：只学X光片诊断
- 能力：只会看X光片
- 问题：换个任务（CT片）就不会了

大模型：全科医生
- 训练：学所有医学知识+文学历史等
- 能力：问什么都能聊，还能专项精进

### **为什么现在才出现？**

1. **数据够了**：互联网积累了足够多文本
    
2. **算力够了**：GPU能快速处理海量计算
    
3. **算法成熟**：Transformer架构效果好
    

## **关键数字**

- **参数量**：一般 > 100亿才算“大”
    
- **训练成本**：GPT-3训练 ≈ 460万美元电费
    
- **上下文长度**：现在能记住几万字的对话
    

## **一句话总结**

> 大模型 = 海量数据 + 巨量参数 + Transformer架构，训练出的“通才”模型