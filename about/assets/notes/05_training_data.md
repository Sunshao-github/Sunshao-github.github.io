# 📚 大模型吃什么“数据粮食”

## **一句话概括**

> 大模型训练数据 = 整个互联网的精华 + 精选书籍 + 优质对话

## **三大数据来源**

### **1. 网络数据（主食）**

- **来源**：网页、论坛、百科、新闻等
    
- **占比**：约60-80%
    
- **特点**：
    
    - 量大：万亿级单词
        
    - 多样：各种主题、文体
        
    - 问题：质量参差不齐
        

### **2. 书籍文献（营养品）**

- **来源**：电子书、学术论文、专利等
    
- **占比**：约10-20%
    
- **优点**：
    
    - 质量高：经过编辑审核
        
    - 逻辑强：结构完整
        
    - 深度：专业知识
        

### **3. 对话数据（调味剂）**

- **来源**：社交媒体、客服记录、论坛对话
    
- **用途**：学习人类对话方式
    
- **难点**：包含大量噪音
    

## **数据构成示例**

text

假设训练数据 = 100顿饭
- 60顿：各种网页（新闻、博客、论坛）
- 20顿：书籍文献（小说、教材、论文）
- 15顿：代码数据（GitHub等）
- 5顿：对话数据（客服记录等）

## **为什么要混合？**

- **网页**：学最新知识、网络用语
    
- **书籍**：学严谨表达、深度思考
    
- **代码**：学逻辑结构、精确性
    
- **对话**：学交流方式、口语化
    

## **数据准备流程**

text

原始数据 → 去重去脏 → 质量过滤 → 安全过滤 → 最终数据
    ↓         ↓          ↓          ↓
去掉重复  去掉乱码   保留优质   去掉有害

## **关键挑战**

1. **质量问题**：网络信息真假混杂
    
2. **偏见问题**：数据反映社会偏见
    
3. **版权问题**：使用数据是否合法
    
4. **安全问题**：防止学习有害内容
    

## **有趣事实**

- GPT-3训练数据 ≈ 读完了整个英语维基百科100遍
    
- 代码数据让模型学会逻辑推理
    
- 多语言数据让模型具备翻译能力